{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "\n",
    "Have you ever wondered how we process and learn information? For example, how does our body process information such that we are able to move our hands or legs? To put it simply, the brain will process information and then send out signals to the rest of the body to trigger certain muscle movements. These signals are transported through the nervous system. One of the main components of the nervous system are neuron cells. These cells work on a threshold basis which means that the signal will only be transferred from cells to cells if it is higher than a certain value or amount. As such, when we decide to move our hands, the signals from the brain will get transferred to the muscle in our hands and not the muscle in our legs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Artificial Neural Networks\n",
    "\n",
    "However, this only explains how we process information. How about the ability of humans to learn? For example, why do we know to stop at a red light or how to kick a ball? It is because we were trained to do so by looking at examples or how other people were doing it. Through these examples, we were able to learn and remember.\n",
    "\n",
    "Would it be great if computers were able to mimic the way humans process and learn information? With artificial neural network, this can be done! Artificial neural networks are able to process and 'learn' complex relationships within datasets. An illustration of a simple neural network is shown below.\n",
    "\n",
    "The basic idea is that we input data into the input layer. Data will be processed in the subsequent hidden layer(s) - we only show one hidden layer on the picture below, but it can be many layers. Each layer is made of multiple artificial neurons which apply functions to the data and pass it on to another hidden layer, finally ending with the output layer.\n",
    "\n",
    "![ANN](assets/ANN.jpg)\n",
    "\n",
    "The illustration above shows a simple neural network with 1 input layer, 1 hidden layer (in between an input layer and an output layer) and 1 output layer. Each circle represent 1 node or 1 neuron. We usually do not discuss the number of nodes at the input layer within the model architecture as the input layer is just the data that is being passed to the model. Thus, the hidden layer has 4 nodes/neurons and the output layer has 1 node/neuron.\n",
    "\n",
    "The output layer will show the results of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the neural network work? How can they be useful for machine learning project? Watch this [video](https://www.youtube.com/watch?v=aircAruvnKk) to find out more about artificial neural networks. Pause the video and take time to try and understand how the neural network works. Note down any interesting information on neural networks on your worksheet. Are you also able to draw out the network (similar to the illustration above) if the network has 1 input layer with 5 nodes, 2 hidden layers with 3 nodes each and 1 output layer with 2 nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network\n",
    "After understanding the different features of an artificial neural network, one question that still remains is how does the network \"learn\"?\n",
    "\n",
    "Take for example a young basketball player who is learning to shoot a 3-point shot. If he shoots and he misses because the shot is too short, the basketball player will adjust and increase the strength of the next shot. If the next shot now is too far right of the basket, the player will again adjust his shot to shoot more towards the center. The player continues to do this until the shot is made. The player then remembers the exact strength and shot direction when shooting 3-point shots in future.\n",
    "\n",
    "This is similar to how neural networks are trained. First, the data is passed through the network and a predicted output is given. This is known as a forward propagation. The predicted output is then compared to the actual output of the data and the differences between the predicted and actual will be passed backwards through the model. During the backward pass, adjustments will be made within the model such that the differences between prediction output and actual output will be reduced. This is known as the backpropagation. After the adjustments are made, data will be passed through from the input layer again and another predicted output will be made. The new predicted output will be compared to the actual output again and the differences will be passed backwards through the model. More adjustments will be made within the model.\n",
    "\n",
    "The process of forward propagation and backpropagation will be repeated until the differences between predicted output and the actual output are minimised. The model is now trained and can be used for prediction of other similar datasets.\n",
    "\n",
    "To have a better visualisation of how backpropagation works, watch this [video](https://www.youtube.com/watch?v=Ilg3gGewQ5U) and take down any information that interests you.\n",
    "\n",
    "**Bonus: You are not required to understand the actual adjustments within the model. However, if you are mathematically inclined or really interested in understanding all the adjustments and have time, you can watch the 2 videos listed below.**\n",
    "\n",
    "- [video 1](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- [video 2](https://www.youtube.com/watch?v=tIeHLnjs5U8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network with the Iris Flower dataset\n",
    "\n",
    "Let's try training the neural network using the Iris Flower dataset! we will start with library imports and data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   Class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/iris.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333\n",
       "std        0.828066     0.435866      1.765298     0.762238\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to split the datset into the x values (features which the model can learn relationships from) and the y values (target values or expected output from the model).\n",
    "\n",
    "#### Standardization\n",
    "\n",
    "We would also have to standardize the dataset. What is standardization for? To understand this, look at the distribution of data above! Notice the different mean and standard deviation. Comparing between these variables will be difficult. Standardization helps us to equalize these various distributions into a common mean and standart deviation, so that we can compare them easily. Refer to the [standardscaler graph here](https://benalexkeen.com/feature-scaling-with-scikit-learn/). See how data changed before and after scaling.\n",
    "\n",
    "This is to allow neural networks to classify easily. The code below will extract out the x values as x_values and also standardise the values. We will extract the y_values later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values = df.drop('Class', axis=1)\n",
    "x_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize = StandardScaler()\n",
    "x_values = standardize.fit_transform(x_values)\n",
    "x_values_df = pd.DataFrame(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>1.500000e+02</td>\n",
       "      <td>1.500000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.736952e-16</td>\n",
       "      <td>-7.815970e-16</td>\n",
       "      <td>-4.263256e-16</td>\n",
       "      <td>-4.736952e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.003350e+00</td>\n",
       "      <td>1.003350e+00</td>\n",
       "      <td>1.003350e+00</td>\n",
       "      <td>1.003350e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.870024e+00</td>\n",
       "      <td>-2.433947e+00</td>\n",
       "      <td>-1.567576e+00</td>\n",
       "      <td>-1.447076e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-9.006812e-01</td>\n",
       "      <td>-5.923730e-01</td>\n",
       "      <td>-1.226552e+00</td>\n",
       "      <td>-1.183812e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-5.250608e-02</td>\n",
       "      <td>-1.319795e-01</td>\n",
       "      <td>3.364776e-01</td>\n",
       "      <td>1.325097e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.745011e-01</td>\n",
       "      <td>5.586108e-01</td>\n",
       "      <td>7.627583e-01</td>\n",
       "      <td>7.906707e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.492019e+00</td>\n",
       "      <td>3.090775e+00</td>\n",
       "      <td>1.785832e+00</td>\n",
       "      <td>1.712096e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3\n",
       "count  1.500000e+02  1.500000e+02  1.500000e+02  1.500000e+02\n",
       "mean  -4.736952e-16 -7.815970e-16 -4.263256e-16 -4.736952e-16\n",
       "std    1.003350e+00  1.003350e+00  1.003350e+00  1.003350e+00\n",
       "min   -1.870024e+00 -2.433947e+00 -1.567576e+00 -1.447076e+00\n",
       "25%   -9.006812e-01 -5.923730e-01 -1.226552e+00 -1.183812e+00\n",
       "50%   -5.250608e-02 -1.319795e-01  3.364776e-01  1.325097e-01\n",
       "75%    6.745011e-01  5.586108e-01  7.627583e-01  7.906707e-01\n",
       "max    2.492019e+00  3.090775e+00  1.785832e+00  1.712096e+00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see how this is different from the original? the mean is almost 0 while the standard deviation is almost 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the neural network\n",
    "\n",
    "We can now build a simple neural network. In order to do so, we will need to import the dense from Tensorflow Keras library and sequential functions from keras.\n",
    "\n",
    "**Sequential**\n",
    "\n",
    "The Sequential model allows you to first create an empty model object, and then add layers to it one after another in sequence.\n",
    "\n",
    "**Dense**\n",
    "\n",
    "A dense layer is simply a layer of neurons in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build a neural network with 1 input layer, 2 hidden layers and 1 output layer. There are no rules to decide how many nodes should be within the hidden layers.\n",
    "\n",
    "For this neural network, we will use 6 hidden nodes for each hidden layer.\n",
    "\n",
    "With regards to the output layer, we should use as many nodes as the number of classes. How many nodes should we use for the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network as model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model.add(Dense(6, input_dim=4, activation='relu'))\n",
    "\n",
    "# Add the next hidden layer with 6 nodes. \n",
    "model.add(Dense(6, activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Optional_: [Comparisons between activation functions](http://www.machineintellegence.com/different-types-of-activation-functions-in-keras/) There are many more activation functions. They are like on-off buttons to allow certain data/input to follow through the neurons or not. You are not expected to know the functions in detail for now.\n",
    "\n",
    "We can also print out the model summary after it has been compiled. Try the code below to see:\n",
    "\n",
    "- The layers and their order in the model.\n",
    "- The output shape of each layer.\n",
    "- The number of parameters (weights) in each layer.\n",
    "- The total number of parameters (weights) in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the y_values are categorical in nature (categories instead of numbers), we have to convert the y_values from categories into numbers before we can train the neural network. For neural networks, if the categories are not numerical groups (for example, 1,2,3,4,etc) we have to perform label encoding (Remember doing this in an earlier notebook?) before doing one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Setosa  Versicolor  Virginica\n",
      "0         1           0          0\n",
      "1         1           0          0\n",
      "2         1           0          0\n",
      "3         1           0          0\n",
      "4         1           0          0\n",
      "..      ...         ...        ...\n",
      "145       0           0          1\n",
      "146       0           0          1\n",
      "147       0           0          1\n",
      "148       0           0          1\n",
      "149       0           0          1\n",
      "\n",
      "[150 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the classes to one-hot encoding\n",
    "y_values = pd.get_dummies(df['Class'])\n",
    "print(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.9719 - accuracy: 0.4000\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.8147 - accuracy: 0.4733\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.5800\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.6084 - accuracy: 0.6133\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5471 - accuracy: 0.6867\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7667\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8200\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4097 - accuracy: 0.8600\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8467\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fa24ea70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model.fit(x_values,y_values,epochs=10,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! You have trained your first neural network. Before we look at the accuracy, we have to understand some of the terms in the output.\n",
    "\n",
    "Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "\n",
    "us/step shows how long the model took to train on each epoch.\n",
    "\n",
    "accuracy shows how accurate the model is.\n",
    "\n",
    "Notice how these numbers change over the different epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model above, what was the accuracy value you've got?\n",
    "\n",
    "Try to see if you can get a better accuracy by adding another hidden layer to the model. The additional hidden layer can have the same number of nodes as the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the neural network as model\n",
    "model1 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model1.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model1.add(Dense(6,activation='relu'))\n",
    "# Add the 3rd hidden layer with 6 nodes. \n",
    "model1.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model1.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 135\n",
      "Trainable params: 135\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 1ms/step - loss: 0.9349 - accuracy: 0.4667\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.8307 - accuracy: 0.7733\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7596 - accuracy: 0.8267\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 837us/step - loss: 0.7083 - accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 898us/step - loss: 0.6635 - accuracy: 0.8400\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 804us/step - loss: 0.6147 - accuracy: 0.8467\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 836us/step - loss: 0.5628 - accuracy: 0.8467\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 872us/step - loss: 0.5105 - accuracy: 0.8600\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4661 - accuracy: 0.8733\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 840us/step - loss: 0.4190 - accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fa5f1390>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model1.fit(x_values,y_values,epochs=10,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding another hidden layer, you should observe that the accuracy seems to be higher than the initial model. As such, this shows that adding more layers can increase the accuracy, but this isn't always the case.\n",
    "\n",
    "**Bonus: You can try other methods to improve the accuracy. How about increasing the number of nodes in the hidden layer? Do you get a higher accuracy if you do so?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 1s 905us/step - loss: 1.0392 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7443 - accuracy: 0.6600\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 0s 866us/step - loss: 0.5320 - accuracy: 0.7133\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 0s 952us/step - loss: 0.4537 - accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 0s 911us/step - loss: 0.3720 - accuracy: 0.8600\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 0s 855us/step - loss: 0.2660 - accuracy: 0.9400\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 0s 951us/step - loss: 0.1866 - accuracy: 0.9467\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 0s 891us/step - loss: 0.1579 - accuracy: 0.9467\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 0s 950us/step - loss: 0.1257 - accuracy: 0.9533\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.1033 - accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fa5c2980>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model2 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or the input layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model2.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model2.add(Dense(10,activation='relu'))\n",
    "# Add the 3rd hidden layer with 6 nodes. \n",
    "model2.add(Dense(10,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model2.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. \n",
    "model2.fit(x_values,y_values,epochs=10,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset\n",
    "\n",
    "From the above, we can see that the neural network can be trained for any number of epochs. This means that the network can keep learning from the same dataset many times. What do you think will happen if the model keeps learning from the same dataset? Do you think the network will be able to obtain very high accuracy by doing so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9733\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9667\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.9733\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9600\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9933\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9800\n",
      "Epoch 7/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9733\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9867\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9800\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9800\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 0s 979us/step - loss: 0.0542 - accuracy: 0.9800\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9867\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9867\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 0s 955us/step - loss: 0.0551 - accuracy: 0.9733\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9667\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9933\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9800\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0446 - accuracy: 0.9800\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 0s 948us/step - loss: 0.0608 - accuracy: 0.9667\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - 0s 899us/step - loss: 0.0513 - accuracy: 0.9867\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.0502 - accuracy: 0.9733\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0482 - accuracy: 0.9800\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0444 - accuracy: 0.9867\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0476 - accuracy: 0.9800\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 0s 941us/step - loss: 0.0436 - accuracy: 0.9867\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 0s 947us/step - loss: 0.0494 - accuracy: 0.9800\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 0s 865us/step - loss: 0.0474 - accuracy: 0.9800\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9800\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9800\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - 0s 970us/step - loss: 0.0512 - accuracy: 0.9800\n",
      "Epoch 31/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9800\n",
      "Epoch 32/40\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0419 - accuracy: 0.9800\n",
      "Epoch 33/40\n",
      "150/150 [==============================] - 0s 893us/step - loss: 0.0458 - accuracy: 0.9867\n",
      "Epoch 34/40\n",
      "150/150 [==============================] - 0s 889us/step - loss: 0.0474 - accuracy: 0.9800\n",
      "Epoch 35/40\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0426 - accuracy: 0.9800\n",
      "Epoch 36/40\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.0427 - accuracy: 0.9800\n",
      "Epoch 37/40\n",
      "150/150 [==============================] - 0s 878us/step - loss: 0.0432 - accuracy: 0.9800\n",
      "Epoch 38/40\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0454 - accuracy: 0.9867\n",
      "Epoch 39/40\n",
      "150/150 [==============================] - 0s 860us/step - loss: 0.0404 - accuracy: 0.9867\n",
      "Epoch 40/40\n",
      "150/150 [==============================] - 0s 929us/step - loss: 0.0436 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fc892ef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_values,y_values,epochs=40,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You were right if you think that the model's accuracy will increase as it continues to learn. You can try it on the same dataset. Run the code below and observe the accuracy. Is it higher than the accuracy from the previous model with the same setting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "150/150 [==============================] - 1s 2ms/step - loss: 1.1488 - accuracy: 0.2933\n",
      "Epoch 2/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.9569 - accuracy: 0.5933\n",
      "Epoch 3/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.7218 - accuracy: 0.7533\n",
      "Epoch 4/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.5851 - accuracy: 0.8000\n",
      "Epoch 5/200\n",
      "150/150 [==============================] - 0s 953us/step - loss: 0.5171 - accuracy: 0.8133\n",
      "Epoch 6/200\n",
      "150/150 [==============================] - 0s 925us/step - loss: 0.4750 - accuracy: 0.8200\n",
      "Epoch 7/200\n",
      "150/150 [==============================] - 0s 983us/step - loss: 0.4444 - accuracy: 0.8200\n",
      "Epoch 8/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.4192 - accuracy: 0.8267\n",
      "Epoch 9/200\n",
      "150/150 [==============================] - 0s 924us/step - loss: 0.3964 - accuracy: 0.8267\n",
      "Epoch 10/200\n",
      "150/150 [==============================] - 0s 938us/step - loss: 0.3763 - accuracy: 0.8333\n",
      "Epoch 11/200\n",
      "150/150 [==============================] - 0s 969us/step - loss: 0.3586 - accuracy: 0.8333\n",
      "Epoch 12/200\n",
      "150/150 [==============================] - 0s 869us/step - loss: 0.3393 - accuracy: 0.8400\n",
      "Epoch 13/200\n",
      "150/150 [==============================] - 0s 939us/step - loss: 0.3215 - accuracy: 0.8533\n",
      "Epoch 14/200\n",
      "150/150 [==============================] - 0s 914us/step - loss: 0.3058 - accuracy: 0.8600\n",
      "Epoch 15/200\n",
      "150/150 [==============================] - 0s 874us/step - loss: 0.2908 - accuracy: 0.8933\n",
      "Epoch 16/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8800\n",
      "Epoch 17/200\n",
      "150/150 [==============================] - 0s 895us/step - loss: 0.2587 - accuracy: 0.8867\n",
      "Epoch 18/200\n",
      "150/150 [==============================] - 0s 806us/step - loss: 0.2390 - accuracy: 0.9133\n",
      "Epoch 19/200\n",
      "150/150 [==============================] - 0s 846us/step - loss: 0.2178 - accuracy: 0.9200\n",
      "Epoch 20/200\n",
      "150/150 [==============================] - 0s 770us/step - loss: 0.1972 - accuracy: 0.9333\n",
      "Epoch 21/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.1803 - accuracy: 0.9467\n",
      "Epoch 22/200\n",
      "150/150 [==============================] - 0s 807us/step - loss: 0.1657 - accuracy: 0.9467\n",
      "Epoch 23/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9667\n",
      "Epoch 24/200\n",
      "150/150 [==============================] - 0s 785us/step - loss: 0.1362 - accuracy: 0.9667\n",
      "Epoch 25/200\n",
      "150/150 [==============================] - 0s 885us/step - loss: 0.1241 - accuracy: 0.9667\n",
      "Epoch 26/200\n",
      "150/150 [==============================] - 0s 831us/step - loss: 0.1135 - accuracy: 0.9667\n",
      "Epoch 27/200\n",
      "150/150 [==============================] - 0s 880us/step - loss: 0.1065 - accuracy: 0.9667\n",
      "Epoch 28/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9733\n",
      "Epoch 29/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9733\n",
      "Epoch 30/200\n",
      "150/150 [==============================] - 0s 866us/step - loss: 0.0831 - accuracy: 0.9733\n",
      "Epoch 31/200\n",
      "150/150 [==============================] - 0s 879us/step - loss: 0.0781 - accuracy: 0.9800\n",
      "Epoch 32/200\n",
      "150/150 [==============================] - 0s 933us/step - loss: 0.0730 - accuracy: 0.9800\n",
      "Epoch 33/200\n",
      "150/150 [==============================] - 0s 826us/step - loss: 0.0687 - accuracy: 0.9800\n",
      "Epoch 34/200\n",
      "150/150 [==============================] - 0s 861us/step - loss: 0.0672 - accuracy: 0.9800\n",
      "Epoch 35/200\n",
      "150/150 [==============================] - 0s 822us/step - loss: 0.0656 - accuracy: 0.9800\n",
      "Epoch 36/200\n",
      "150/150 [==============================] - 0s 960us/step - loss: 0.0617 - accuracy: 0.9800\n",
      "Epoch 37/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9800\n",
      "Epoch 38/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.0596 - accuracy: 0.9800\n",
      "Epoch 39/200\n",
      "150/150 [==============================] - 0s 974us/step - loss: 0.0579 - accuracy: 0.9800\n",
      "Epoch 40/200\n",
      "150/150 [==============================] - 0s 841us/step - loss: 0.0576 - accuracy: 0.9867\n",
      "Epoch 41/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9800\n",
      "Epoch 42/200\n",
      "150/150 [==============================] - 0s 818us/step - loss: 0.0549 - accuracy: 0.9800\n",
      "Epoch 43/200\n",
      "150/150 [==============================] - 0s 997us/step - loss: 0.0539 - accuracy: 0.9800\n",
      "Epoch 44/200\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.0548 - accuracy: 0.9800\n",
      "Epoch 45/200\n",
      "150/150 [==============================] - 0s 1000us/step - loss: 0.0518 - accuracy: 0.9800\n",
      "Epoch 46/200\n",
      "150/150 [==============================] - 0s 798us/step - loss: 0.0534 - accuracy: 0.9800\n",
      "Epoch 47/200\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.0514 - accuracy: 0.9867\n",
      "Epoch 48/200\n",
      "150/150 [==============================] - 0s 856us/step - loss: 0.0527 - accuracy: 0.9800\n",
      "Epoch 49/200\n",
      "150/150 [==============================] - 0s 985us/step - loss: 0.0500 - accuracy: 0.9867\n",
      "Epoch 50/200\n",
      "150/150 [==============================] - 0s 827us/step - loss: 0.0493 - accuracy: 0.9800\n",
      "Epoch 51/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9800\n",
      "Epoch 52/200\n",
      "150/150 [==============================] - 0s 843us/step - loss: 0.0488 - accuracy: 0.9867\n",
      "Epoch 53/200\n",
      "150/150 [==============================] - 0s 799us/step - loss: 0.0478 - accuracy: 0.9933\n",
      "Epoch 54/200\n",
      "150/150 [==============================] - 0s 827us/step - loss: 0.0478 - accuracy: 0.9800\n",
      "Epoch 55/200\n",
      "150/150 [==============================] - 0s 935us/step - loss: 0.0477 - accuracy: 0.9867\n",
      "Epoch 56/200\n",
      "150/150 [==============================] - 0s 826us/step - loss: 0.0477 - accuracy: 0.9800\n",
      "Epoch 57/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.0475 - accuracy: 0.9800\n",
      "Epoch 58/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9867\n",
      "Epoch 59/200\n",
      "150/150 [==============================] - 0s 888us/step - loss: 0.0457 - accuracy: 0.9800\n",
      "Epoch 60/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.0457 - accuracy: 0.9867\n",
      "Epoch 61/200\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0453 - accuracy: 0.9800\n",
      "Epoch 62/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0456 - accuracy: 0.9867\n",
      "Epoch 63/200\n",
      "150/150 [==============================] - 0s 814us/step - loss: 0.0446 - accuracy: 0.9867\n",
      "Epoch 64/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.0463 - accuracy: 0.9867\n",
      "Epoch 65/200\n",
      "150/150 [==============================] - 0s 812us/step - loss: 0.0453 - accuracy: 0.9800\n",
      "Epoch 66/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9867\n",
      "Epoch 67/200\n",
      "150/150 [==============================] - 0s 797us/step - loss: 0.0438 - accuracy: 0.9867\n",
      "Epoch 68/200\n",
      "150/150 [==============================] - 0s 806us/step - loss: 0.0430 - accuracy: 0.9867\n",
      "Epoch 69/200\n",
      "150/150 [==============================] - 0s 808us/step - loss: 0.0433 - accuracy: 0.9800\n",
      "Epoch 70/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9933\n",
      "Epoch 71/200\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.0411 - accuracy: 0.9867\n",
      "Epoch 72/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0417 - accuracy: 0.9867\n",
      "Epoch 73/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0430 - accuracy: 0.9867\n",
      "Epoch 74/200\n",
      "150/150 [==============================] - 0s 859us/step - loss: 0.0404 - accuracy: 0.9933\n",
      "Epoch 75/200\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.0416 - accuracy: 0.9867\n",
      "Epoch 76/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9867\n",
      "Epoch 77/200\n",
      "150/150 [==============================] - 0s 846us/step - loss: 0.0422 - accuracy: 0.9933\n",
      "Epoch 78/200\n",
      "150/150 [==============================] - 0s 871us/step - loss: 0.0442 - accuracy: 0.9800\n",
      "Epoch 79/200\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0400 - accuracy: 0.9933\n",
      "Epoch 80/200\n",
      "150/150 [==============================] - 0s 824us/step - loss: 0.0417 - accuracy: 0.9933\n",
      "Epoch 81/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9867\n",
      "Epoch 82/200\n",
      "150/150 [==============================] - 0s 941us/step - loss: 0.0414 - accuracy: 0.9867\n",
      "Epoch 83/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0414 - accuracy: 0.9933\n",
      "Epoch 84/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9933\n",
      "Epoch 85/200\n",
      "150/150 [==============================] - 0s 952us/step - loss: 0.0405 - accuracy: 0.9867\n",
      "Epoch 86/200\n",
      "150/150 [==============================] - 0s 860us/step - loss: 0.0400 - accuracy: 0.9933\n",
      "Epoch 87/200\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0412 - accuracy: 0.9867\n",
      "Epoch 88/200\n",
      "150/150 [==============================] - 0s 847us/step - loss: 0.0412 - accuracy: 0.9867\n",
      "Epoch 89/200\n",
      "150/150 [==============================] - 0s 880us/step - loss: 0.0382 - accuracy: 0.9933\n",
      "Epoch 90/200\n",
      "150/150 [==============================] - 0s 932us/step - loss: 0.0388 - accuracy: 0.9933\n",
      "Epoch 91/200\n",
      "150/150 [==============================] - 0s 922us/step - loss: 0.0393 - accuracy: 0.9933\n",
      "Epoch 92/200\n",
      "150/150 [==============================] - 0s 876us/step - loss: 0.0393 - accuracy: 0.9933\n",
      "Epoch 93/200\n",
      "150/150 [==============================] - 0s 789us/step - loss: 0.0381 - accuracy: 0.9933\n",
      "Epoch 94/200\n",
      "150/150 [==============================] - 0s 914us/step - loss: 0.0370 - accuracy: 0.9933\n",
      "Epoch 95/200\n",
      "150/150 [==============================] - 0s 853us/step - loss: 0.0382 - accuracy: 0.9867\n",
      "Epoch 96/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0379 - accuracy: 0.9867\n",
      "Epoch 97/200\n",
      "150/150 [==============================] - 0s 768us/step - loss: 0.0386 - accuracy: 0.9933\n",
      "Epoch 98/200\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0387 - accuracy: 0.9933\n",
      "Epoch 99/200\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0365 - accuracy: 0.9933\n",
      "Epoch 100/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9933\n",
      "Epoch 101/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9933\n",
      "Epoch 102/200\n",
      "150/150 [==============================] - 0s 841us/step - loss: 0.0386 - accuracy: 0.9867\n",
      "Epoch 103/200\n",
      "150/150 [==============================] - 0s 894us/step - loss: 0.0358 - accuracy: 0.9933\n",
      "Epoch 104/200\n",
      "150/150 [==============================] - 0s 869us/step - loss: 0.0358 - accuracy: 0.9933\n",
      "Epoch 105/200\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.0366 - accuracy: 0.9933\n",
      "Epoch 106/200\n",
      "150/150 [==============================] - 0s 804us/step - loss: 0.0381 - accuracy: 0.9867\n",
      "Epoch 107/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9867\n",
      "Epoch 108/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0357 - accuracy: 0.9933\n",
      "Epoch 109/200\n",
      "150/150 [==============================] - 0s 897us/step - loss: 0.0351 - accuracy: 0.9933\n",
      "Epoch 110/200\n",
      "150/150 [==============================] - 0s 865us/step - loss: 0.0361 - accuracy: 0.9933\n",
      "Epoch 111/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9867\n",
      "Epoch 112/200\n",
      "150/150 [==============================] - 0s 934us/step - loss: 0.0349 - accuracy: 0.9933\n",
      "Epoch 113/200\n",
      "150/150 [==============================] - 0s 882us/step - loss: 0.0361 - accuracy: 0.9933\n",
      "Epoch 114/200\n",
      "150/150 [==============================] - 0s 866us/step - loss: 0.0357 - accuracy: 0.9800\n",
      "Epoch 115/200\n",
      "150/150 [==============================] - 0s 861us/step - loss: 0.0360 - accuracy: 0.9933\n",
      "Epoch 116/200\n",
      "150/150 [==============================] - 0s 830us/step - loss: 0.0353 - accuracy: 0.9933\n",
      "Epoch 117/200\n",
      "150/150 [==============================] - 0s 814us/step - loss: 0.0345 - accuracy: 0.9933\n",
      "Epoch 118/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0346 - accuracy: 0.9933\n",
      "Epoch 119/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9867\n",
      "Epoch 120/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9933\n",
      "Epoch 121/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0345 - accuracy: 0.9933\n",
      "Epoch 122/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0347 - accuracy: 0.9933\n",
      "Epoch 123/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9867\n",
      "Epoch 124/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9933\n",
      "Epoch 125/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9933\n",
      "Epoch 126/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9867\n",
      "Epoch 127/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0336 - accuracy: 0.9867\n",
      "Epoch 128/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9933\n",
      "Epoch 129/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9933\n",
      "Epoch 130/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9933\n",
      "Epoch 131/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9867\n",
      "Epoch 132/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9867\n",
      "Epoch 133/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9867\n",
      "Epoch 134/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9933\n",
      "Epoch 135/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9867\n",
      "Epoch 136/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0337 - accuracy: 0.9867\n",
      "Epoch 137/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9933\n",
      "Epoch 138/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0311 - accuracy: 0.9933\n",
      "Epoch 139/200\n",
      "150/150 [==============================] - 0s 840us/step - loss: 0.0328 - accuracy: 0.9933\n",
      "Epoch 140/200\n",
      "150/150 [==============================] - 0s 971us/step - loss: 0.0332 - accuracy: 0.9933\n",
      "Epoch 141/200\n",
      "150/150 [==============================] - 0s 906us/step - loss: 0.0314 - accuracy: 0.9933\n",
      "Epoch 142/200\n",
      "150/150 [==============================] - 0s 847us/step - loss: 0.0343 - accuracy: 0.9867\n",
      "Epoch 143/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9933\n",
      "Epoch 144/200\n",
      "150/150 [==============================] - 0s 979us/step - loss: 0.0325 - accuracy: 0.9867\n",
      "Epoch 145/200\n",
      "150/150 [==============================] - 0s 786us/step - loss: 0.0317 - accuracy: 0.9933\n",
      "Epoch 146/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9867\n",
      "Epoch 147/200\n",
      "150/150 [==============================] - 0s 939us/step - loss: 0.0319 - accuracy: 0.9933\n",
      "Epoch 148/200\n",
      "150/150 [==============================] - 0s 925us/step - loss: 0.0317 - accuracy: 0.9933\n",
      "Epoch 149/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0318 - accuracy: 0.9867\n",
      "Epoch 150/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.9933\n",
      "Epoch 151/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.9933\n",
      "Epoch 152/200\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0315 - accuracy: 0.9867\n",
      "Epoch 153/200\n",
      "150/150 [==============================] - 0s 847us/step - loss: 0.0311 - accuracy: 0.9867\n",
      "Epoch 154/200\n",
      "150/150 [==============================] - 0s 873us/step - loss: 0.0321 - accuracy: 0.9933\n",
      "Epoch 155/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9933\n",
      "Epoch 156/200\n",
      "150/150 [==============================] - 0s 890us/step - loss: 0.0310 - accuracy: 0.9933\n",
      "Epoch 157/200\n",
      "150/150 [==============================] - 0s 930us/step - loss: 0.0322 - accuracy: 0.9867\n",
      "Epoch 158/200\n",
      "150/150 [==============================] - 0s 907us/step - loss: 0.0308 - accuracy: 0.9933\n",
      "Epoch 159/200\n",
      "150/150 [==============================] - 0s 981us/step - loss: 0.0314 - accuracy: 0.9867\n",
      "Epoch 160/200\n",
      "150/150 [==============================] - 0s 805us/step - loss: 0.0300 - accuracy: 0.9933\n",
      "Epoch 161/200\n",
      "150/150 [==============================] - 0s 836us/step - loss: 0.0308 - accuracy: 0.9867\n",
      "Epoch 162/200\n",
      "150/150 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9867\n",
      "Epoch 163/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9933\n",
      "Epoch 164/200\n",
      "150/150 [==============================] - 0s 885us/step - loss: 0.0310 - accuracy: 0.9867\n",
      "Epoch 165/200\n",
      "150/150 [==============================] - 0s 878us/step - loss: 0.0306 - accuracy: 0.9933\n",
      "Epoch 166/200\n",
      "150/150 [==============================] - 0s 975us/step - loss: 0.0301 - accuracy: 0.9867\n",
      "Epoch 167/200\n",
      "150/150 [==============================] - 0s 869us/step - loss: 0.0304 - accuracy: 0.9867\n",
      "Epoch 168/200\n",
      "150/150 [==============================] - 0s 915us/step - loss: 0.0310 - accuracy: 0.9933\n",
      "Epoch 169/200\n",
      "150/150 [==============================] - 0s 841us/step - loss: 0.0290 - accuracy: 0.9867\n",
      "Epoch 170/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0310 - accuracy: 0.9933\n",
      "Epoch 171/200\n",
      "150/150 [==============================] - 0s 942us/step - loss: 0.0307 - accuracy: 0.9867\n",
      "Epoch 172/200\n",
      "150/150 [==============================] - 0s 817us/step - loss: 0.0313 - accuracy: 0.9867\n",
      "Epoch 173/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0294 - accuracy: 0.9933\n",
      "Epoch 174/200\n",
      "150/150 [==============================] - 0s 886us/step - loss: 0.0300 - accuracy: 0.9933\n",
      "Epoch 175/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0299 - accuracy: 0.9933\n",
      "Epoch 176/200\n",
      "150/150 [==============================] - 0s 870us/step - loss: 0.0290 - accuracy: 0.9867\n",
      "Epoch 177/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.9933\n",
      "Epoch 178/200\n",
      "150/150 [==============================] - 0s 955us/step - loss: 0.0297 - accuracy: 0.9867\n",
      "Epoch 179/200\n",
      "150/150 [==============================] - 0s 848us/step - loss: 0.0288 - accuracy: 0.9933\n",
      "Epoch 180/200\n",
      "150/150 [==============================] - 0s 904us/step - loss: 0.0287 - accuracy: 0.9867\n",
      "Epoch 181/200\n",
      "150/150 [==============================] - 0s 850us/step - loss: 0.0298 - accuracy: 0.9933\n",
      "Epoch 182/200\n",
      "150/150 [==============================] - 0s 853us/step - loss: 0.0302 - accuracy: 0.9933\n",
      "Epoch 183/200\n",
      "150/150 [==============================] - 0s 941us/step - loss: 0.0281 - accuracy: 0.9867\n",
      "Epoch 184/200\n",
      "150/150 [==============================] - 0s 879us/step - loss: 0.0289 - accuracy: 0.9867\n",
      "Epoch 185/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0302 - accuracy: 0.9867\n",
      "Epoch 186/200\n",
      "150/150 [==============================] - 0s 919us/step - loss: 0.0277 - accuracy: 0.9933\n",
      "Epoch 187/200\n",
      "150/150 [==============================] - 0s 872us/step - loss: 0.0293 - accuracy: 0.9933\n",
      "Epoch 188/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9933\n",
      "Epoch 189/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 0.9867\n",
      "Epoch 190/200\n",
      "150/150 [==============================] - 0s 940us/step - loss: 0.0278 - accuracy: 0.9933\n",
      "Epoch 191/200\n",
      "150/150 [==============================] - 0s 907us/step - loss: 0.0299 - accuracy: 0.9867\n",
      "Epoch 192/200\n",
      "150/150 [==============================] - 0s 961us/step - loss: 0.0264 - accuracy: 0.9933\n",
      "Epoch 193/200\n",
      "150/150 [==============================] - 0s 900us/step - loss: 0.0275 - accuracy: 0.9867\n",
      "Epoch 194/200\n",
      "150/150 [==============================] - 0s 864us/step - loss: 0.0270 - accuracy: 0.9933\n",
      "Epoch 195/200\n",
      "150/150 [==============================] - 0s 916us/step - loss: 0.0287 - accuracy: 0.9933\n",
      "Epoch 196/200\n",
      "150/150 [==============================] - 0s 828us/step - loss: 0.0283 - accuracy: 0.9867\n",
      "Epoch 197/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9933\n",
      "Epoch 198/200\n",
      "150/150 [==============================] - 0s 874us/step - loss: 0.0256 - accuracy: 0.9933\n",
      "Epoch 199/200\n",
      "150/150 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 0.9933\n",
      "Epoch 200/200\n",
      "150/150 [==============================] - 0s 917us/step - loss: 0.0265 - accuracy: 0.9933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fc893be0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model4 = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or input_layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model4.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model4.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Train model with x_values and y_values. Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "model4.fit(x_values,y_values,epochs=200,shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe from the printout that the accuracy score is now above 99%. Wow! It seems amazing that just increasing the number of epochs will allow the accuracy to increase. Do you think it is good that the model is so highly accuracte on the data that it has trained on? Just imagine, will a soccer player do well in a match if he/she only trains extremely hard on scoring a goal from a specific spot on the field? Or will a baker be able to bake a delicious cake based on a customer's request if he/she only learns how to bake a cake that is a specific flavour, shape and size?\n",
    "\n",
    "The soccer player will not perform well in a match as he/she may not be able to shoot accurately from other parts of the field. The baker will not be able to bake a delicious cake as he/she will only know 1 specific flavour.\n",
    "\n",
    "The idea behind this question is the concept of overfitting. If the model overfits, it will not be able to generalise to properly predict data that it has not seen before.\n",
    "\n",
    "This is the concept of overfitting and also applies to all machine learning techniques. Once the technique has fitted too acurrately on the dataset, the trained technique will not be able to generalise to other data that it has not seen before. As such, we usually only train the technique on a fraction of the dataset that we have and keep the remainder as a test or validation set to see if the model has overfitted. When training on the training set, the accuracy of the model on the test set should increase. However, at the point of overfitting, the accuracy on the test set will start to decrease. If you see that the test accuracy has begin to increase after a certain epoch, you should not train the model any further.\n",
    "\n",
    "Let us apply the split between a train and a test set to the dataset that we are using here.\n",
    "\n",
    "First, we need to decide how much data we want to keep and prevent the model from training on. Usually, we withold about 20% to 30% of the dataset. In this example, we will keep 25 percent of the dataset as the test/validation set. We can use the train_test_split function from the sklearn library. Try the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in x_train: 120\n",
      "Number of rows in x_test: 30\n",
      "Number of rows in y_train: 120\n",
      "Number of rows in y_test: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract out original x_values from the dataframe df. \n",
    "# We have to re-extract the x_values as the standardisation should only be based on the data that the model will train with.\n",
    "# Thus, we have to split the data first before standardising.\n",
    "x_values = df[['sepal_length','sepal_width','petal_length','petal_width']]\n",
    "\n",
    "# Test_size=0.20 indicates that 20% of the datapoints will be in x_test and y_test whereas 70% will be in x_train and y_train.\n",
    "# random_state=10 is used to ensure that the split is the same everytime you run the code below. \n",
    "# This is because the split is done randomly everytime. The same random_state is the only way to ensure the same split everytime.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_values,y_values,test_size=0.20,random_state=10)\n",
    "\n",
    "# Check the number of rows in x_train, x_test, y_train and y_test\n",
    "print(\"Number of rows in x_train:\", x_train.shape[0])\n",
    "print(\"Number of rows in x_test:\", x_test.shape[0])\n",
    "print(\"Number of rows in y_train:\", y_train.shape[0])\n",
    "print(\"Number of rows in y_test:\", y_test.shape[0])\n",
    "\n",
    "# We can now standardise the x values.\n",
    "# Initialise the StandardScaler\n",
    "standardise = StandardScaler()\n",
    "\n",
    "# Standardise the x_train values using .fit_transform\n",
    "x_train = standardise.fit_transform(x_train)\n",
    "\n",
    "# Standardise the x_test values using .transform. \n",
    "# There is no need to fit the data as the standardisation should be the same as that of x_train.\n",
    "x_test = standardise.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 6)                 30        \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 6)                 42        \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 3)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93\n",
      "Trainable params: 93\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initialise the neural network as model\n",
    "model_val = Sequential()\n",
    "\n",
    "# Add the first hidden layer with 6 nodes. Input_dim refers to the number of columns/number of features in x_values or input_layer.\n",
    "# Activation refers to how the nodes/neurons are activated. We will use relu. Other common activations are 'sigmoid' and 'tanh'\n",
    "model_val.add(Dense(6,input_dim=4,activation='relu'))\n",
    "\n",
    "# Add the hidden layer with 6 nodes. \n",
    "model_val.add(Dense(6,activation='relu'))\n",
    "\n",
    "# Add the output layer with 3 nodes. The activation used has to be 'softmax'. Softmax is used when you are dealing with categorical outputs or targets. \n",
    "model_val.add(Dense(3,activation='softmax'))\n",
    "\n",
    "# Compile the model together. The optimizer refers to the method to make the adjustment within the model. Loss refers to how the difference between the predicted out \n",
    "model_val.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_val.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "120/120 [==============================] - 1s 4ms/step - loss: 0.9442 - accuracy: 0.5500 - val_loss: 0.8796 - val_accuracy: 0.5333\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.7694 - accuracy: 0.6583 - val_loss: 0.7364 - val_accuracy: 0.5667\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.6279 - accuracy: 0.6917 - val_loss: 0.6195 - val_accuracy: 0.5667\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.5292 - accuracy: 0.7000 - val_loss: 0.5356 - val_accuracy: 0.6000\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.7667 - val_loss: 0.4807 - val_accuracy: 0.7333\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.8250 - val_loss: 0.4371 - val_accuracy: 0.7333\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8417 - val_loss: 0.4034 - val_accuracy: 0.7667\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3582 - accuracy: 0.8417 - val_loss: 0.3752 - val_accuracy: 0.8667\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.3363 - accuracy: 0.8583 - val_loss: 0.3506 - val_accuracy: 0.9000\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8667 - val_loss: 0.3267 - val_accuracy: 0.9000\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8667 - val_loss: 0.3105 - val_accuracy: 0.9000\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8667 - val_loss: 0.2900 - val_accuracy: 0.9000\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.2707 - accuracy: 0.8833 - val_loss: 0.2738 - val_accuracy: 0.9000\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.8917 - val_loss: 0.2524 - val_accuracy: 0.9667\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.8833 - val_loss: 0.2370 - val_accuracy: 0.9667\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.9250 - val_loss: 0.2199 - val_accuracy: 0.9667\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.2165 - accuracy: 0.9167 - val_loss: 0.2133 - val_accuracy: 0.9333\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.2015 - accuracy: 0.9167 - val_loss: 0.1823 - val_accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9417 - val_loss: 0.1665 - val_accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1704 - accuracy: 0.9500 - val_loss: 0.1537 - val_accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9500 - val_loss: 0.1403 - val_accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9500 - val_loss: 0.1329 - val_accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9667 - val_loss: 0.1234 - val_accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9583 - val_loss: 0.1171 - val_accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1215 - accuracy: 0.9750 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.9667 - val_loss: 0.1014 - val_accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.1106 - accuracy: 0.9750 - val_loss: 0.0919 - val_accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 0s 3ms/step - loss: 0.1035 - accuracy: 0.9583 - val_loss: 0.1000 - val_accuracy: 0.9667\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0972 - accuracy: 0.9750 - val_loss: 0.0819 - val_accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9667 - val_loss: 0.0819 - val_accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9750 - val_loss: 0.0845 - val_accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9667 - val_loss: 0.0774 - val_accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9667 - val_loss: 0.0746 - val_accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9667 - val_loss: 0.0709 - val_accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9667 - val_loss: 0.0658 - val_accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9750 - val_loss: 0.0645 - val_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9667 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9750 - val_loss: 0.0573 - val_accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9750 - val_loss: 0.0600 - val_accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9750 - val_loss: 0.0611 - val_accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9833 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9750 - val_loss: 0.0515 - val_accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9833 - val_loss: 0.0537 - val_accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9667 - val_loss: 0.0536 - val_accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 0s 2ms/step - loss: 0.0621 - accuracy: 0.9750 - val_loss: 0.0548 - val_accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9833 - val_loss: 0.0495 - val_accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9833 - val_loss: 0.0479 - val_accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9750 - val_loss: 0.0461 - val_accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9750 - val_loss: 0.0421 - val_accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "120/120 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 0.0445 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211fdab2a70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with x_values and y_values. \n",
    "# Epochs refer to the number of times the full dataset will be used to train the model.\n",
    "# Shuffle = True tells the model to randomise the arrangement of the dataset after each epoch. This will allow the model to learn.\n",
    "# Validation_data will allow us to input in the test/validation datasets. This will allow us to see the model accuracy on the test/validation set.\n",
    "model_val.fit(x_train,y_train,epochs=50,shuffle=True,validation_data=(x_test,y_test), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the printout now shows the validation accuracy as well? Is the validation accuracy higher or lower than the training accuracy?\n",
    "\n",
    "We can also use the model to identify flower types for newly gathered data. For example, imagine that your friend has measured the ``sepal_length``, ``sepal_width``, ``petal_length`` and ``petal_width`` of some flowers and saved the data in a file called ``\"iris_predict.data\"``. Your friend wants to find the flower types for these flowers based on the values measured. Would you be able to use your model to help your friend? What are the flower types that your friend measured? Try the codes below! You can use ``model.predict`` method to obtain the flower types for your friend. Additionally, the values returned by the ``.predict`` method will be the probability of each flower type that the model thinks should be assigned to each data row. As such, the higher the probablity, the more confident the model is that the flower type predicted is correct. For example, if the predicted values are very high in the second column, then the model thinks that the flower type is versicolor. You will also need to scale your data before obtaining the flower types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv('datasets/iris_predict.csv')\n",
    "x = df2.drop('Class', axis=1)\n",
    "x_scaled = standardise.transform(x)\n",
    "y_pred = model_val.predict(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.98875320e-01 1.12366502e-03 9.10445920e-07]\n",
      " [6.28031557e-05 5.40373147e-01 4.59564060e-01]\n",
      " [4.78343536e-05 9.37768579e-01 6.21835366e-02]\n",
      " [9.17562772e-08 7.06989737e-03 9.92929995e-01]\n",
      " [1.19578205e-02 9.47605550e-01 4.04366516e-02]\n",
      " [1.75949780e-03 8.62990618e-01 1.35249913e-01]\n",
      " [5.31231053e-04 9.96929348e-01 2.53946520e-03]\n",
      " [1.17281536e-06 9.44635482e-04 9.99054134e-01]\n",
      " [1.15284696e-02 9.88050640e-01 4.20994504e-04]\n",
      " [5.74647538e-06 2.09561363e-02 9.79038119e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the flower types for each data row? You can use the [argmax](https://www.geeksforgeeks.org/numpy-argmax-python/) function to help you identify the flower types from y_pred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 2, 1, 1, 1, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "flower_types = []\n",
    "for ii in range(0,y_pred.shape[0]):\n",
    "    flower_types.append(np.argmax(y_pred[ii,:]))\n",
    "print(flower_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - setosa\n",
    "1 - versicolor\n",
    "2 - virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have successfully learnt how to create an artificial neural network model and trained it. Artficial neural networks are extremely powerful tools that can be used on very large datasets. For example, if you have a couple of hundred columns/features and over 100000 data points, it may be beneficial to use artificial neural networks instead of other machine learning techniques. Just remember, there are no strict rules on the number of nodes in the hidden layer or the number of hidden layers. Experiment with different neural networks to see which one provides the best result for your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
