{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept of Cross-Validation\n",
    "\n",
    "Machine learning is an iterative process.You will face choices about predictive variables to use, what types of models to use, what arguments to\n",
    "supply those models, etc. We make these choices in a data-driven way by measuring model quality of\n",
    "various alternatives.\n",
    "\n",
    "You've already learned to use ``train_test_split`` to split the data, so you can measure model quality on the\n",
    "test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to\n",
    "train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it\n",
    "takes longer to run.\n",
    "\n",
    "### The Shortcoming of Train-Test Split\n",
    "\n",
    "Imagine you have a dataset with 5000 rows. The ``train_test_split`` function has an argument for ``test_size``\n",
    "that you can use to decide how many rows go to the training set and how many go to the test set. The\n",
    "larger the test set, the more reliable your measures of model quality will be. At an extreme, you could\n",
    "imagine having only 1 row of data in the test set. If you compare alternative models, which one makes\n",
    "the best predictions on a single data point will be mostly a matter of luck.\n",
    "\n",
    "You will typically keep about 20% as a test dataset. But even with 1000 rows in the test set, there's some\n",
    "random chance in determining model scores. A model might do well on one set of 1000 rows, even if it\n",
    "would be inaccurate on a different 1000 rows. The larger the test set, the less randomness (aka \"noise\")\n",
    "there is in our measure of model quality.\n",
    "\n",
    "### The Cross-Validation Procedure\n",
    "\n",
    "In cross-validation, we run our modeling process on different subsets of the data to get multiple\n",
    "measures of model quality. For example, we could have 5 folds or experiments. We divide the data into\n",
    "5 pieces, each being 20% of the full dataset.\n",
    "\n",
    "![Cross val](assets/crossval.png)\n",
    "\n",
    "We run an experiment called experiment 1 which uses the first fold as a holdout set, and everything\n",
    "else as training data. This gives us a measure of model quality based on a 20% holdout set, much as we\n",
    "got from using the simple train-test split.\n",
    "\n",
    "We then run a second experiment, where we hold out data from the second fold (using everything\n",
    "except the 2nd fold for training the model.) This gives us a second estimate of model quality. We repeat\n",
    "this process, using every fold once as the holdout. Putting this together, 100% of the data is used as a\n",
    "holdout at some point.\n",
    "\n",
    "Returning to our example above from train-test split, if we have 5000 rows of data, we end up with a\n",
    "measure of model quality based on 5000 rows of holdout (even if we don't use all 5000 rows\n",
    "simultaneously.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs Between Cross-Validation and Train-Test Split\n",
    "\n",
    "Cross-validation gives a more accurate measure of model quality, which is especially important if you\n",
    "are making a lot of modeling decisions. However, it can take more time to run, because it estimates\n",
    "models once for each fold. So it is doing more total work.\n",
    "\n",
    "Given these tradeoffs, when should you use each approach? On small datasets, the extra computational\n",
    "burden of running cross-validation isn't a big deal. These are also the problems where model quality\n",
    "scores would be least reliable with train-test split. So, if your dataset is smaller, you should run crossvalidation.\n",
    "\n",
    "For the same reasons, a simple train-test split is sufficient for larger datasets. It will run faster, and you\n",
    "may have enough data that there's little need to re-use some of it for holdout.\n",
    "\n",
    "There's no simple threshold for what constitutes a large vs small dataset. If your model takes a couple\n",
    "minute or less to run, it's probably worth switching to cross-validation. If your model takes much longer\n",
    "to run, cross-validation may slow down your workflow more than it's worth.\n",
    "\n",
    "Alternatively, you can run cross-validation and see if the scores for each experiment seem close. If each\n",
    "experiment gives the same results, train-test split is probably sufficient.\n",
    "\n",
    "Now, let's see an example of how this can be done with the iris dataset. We will start with library imports and data imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width   Class\n",
       "0           5.1          3.5           1.4          0.2  Setosa\n",
       "1           4.9          3.0           1.4          0.2  Setosa\n",
       "2           4.7          3.2           1.3          0.2  Setosa\n",
       "3           4.6          3.1           1.5          0.2  Setosa\n",
       "4           5.0          3.6           1.4          0.2  Setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered the data-preprocessing steps multiple times before, so now we will directly make a feature dataframe and labels dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.drop(\"Class\", axis=1)\n",
    "y_train = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a model, lets use an ANN for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before proceeding, we need to do k-fold cross validation where k is the number of folds we want. sklearn has this function already and we will be using that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Folds cross-validator\n",
    "\n",
    "Provides train/test indices to split data in train/test sets. Split\n",
    "dataset into k consecutive folds (without shuffling by default). Each fold is then used once as a validation while the k - 1 remaining\n",
    "folds form the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=5 ,shuffle=True, random_state=42) # shuffle=True is important! Otherwise, the data is not shuffled and the CV is not random\n",
    "# random_state=42 is just for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will use the indices to split the data for each fold and then fit a model, and evaluate the performance. Lastly we will take the average score of all the folds. This is a way to measure how good our model is performing. Once we are happy with the average metrics from K-Fold Cross Validation, we can finalize the model architecture and hyperparameters and train it on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for fold : 1\n",
      "Evaluating model\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1383 - accuracy: 0.9667\n",
      "Fold 1: accuracy of 96.66666388511658\n",
      "\n",
      "\n",
      "Training for fold : 2\n",
      "Evaluating model\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.1167 - accuracy: 0.9667\n",
      "Fold 2: accuracy of 96.66666388511658\n",
      "\n",
      "\n",
      "Training for fold : 3\n",
      "Evaluating model\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.1329 - accuracy: 0.9667\n",
      "Fold 3: accuracy of 96.66666388511658\n",
      "\n",
      "\n",
      "Training for fold : 4\n",
      "Evaluating model\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.2148 - accuracy: 0.9333\n",
      "Fold 4: accuracy of 93.33333373069763\n",
      "\n",
      "\n",
      "Training for fold : 5\n",
      "Evaluating model\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.1551 - accuracy: 1.0000\n",
      "Fold 5: accuracy of 100.0\n",
      "\n",
      "\n",
      "Average accuracy: 96.66666507720947\n",
      "Standard Deviation: 2.1081849811218007\n"
     ]
    }
   ],
   "source": [
    "scores_list = []\n",
    "for i, (train_index, test_index) in enumerate(kfolds.split(X_train)):\n",
    "    X_train_kf, X_test_kf = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_kf, y_test_kf = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, activation='relu')) # one layer with 8 neurons\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(f\"Training for fold : {i + 1}\")\n",
    "    model.fit(X_train_kf, pd.get_dummies(y_train_kf), epochs=50, batch_size=1, verbose = 0)\n",
    "\n",
    "    print(\"Evaluating model\")\n",
    "    scores = model.evaluate(X_test_kf, pd.get_dummies(y_test_kf))\n",
    "    print(f\"Fold {i + 1}: {model.metrics_names[1]} of {scores[1]*100}\")\n",
    "    scores_list.append(scores[1]*100)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(f\"Average accuracy: {np.mean(scores_list)}\")\n",
    "print(f\"Standard Deviation: {np.std(scores_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cool, average accuracy of 96%, you can try some other models to see if this gets better using k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
